# -*- coding: utf-8 -*-
"""campus

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AWHakNsY92tW-RQlniwGzgfGdflUWLyO
"""

import numpy as np
import pandas as pd

# data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# machine learning
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn import preprocessing



data = pd.read_csv("/content/Placement.csv")

# Check for missing values and handle them
missing_values = data.isnull().sum()
print(f"Missing values in each column:\n{missing_values}")

# If there are missing values, decide on how to handle them
# For example, filling numerical columns with the mean or mode for categorical columns
data['mba_p'].fillna(data['mba_p'].mean(), inplace=True)
data['salary'].fillna(data['salary'].median(), inplace=True)  # Fill missing salary with median
data.dropna(subset=['status'], inplace=True)  # Drop rows with missing target 'status'

# Check for data types and basic checks
print("\nData types and unique values:")
print(data.dtypes)
print(data.nunique())

# Define numerical_cols before using it
numerical_cols = ['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p', 'salary'] # Assuming these are your numerical columns

# Visualizing distributions with boxplots to check for outliers
plt.figure(figsize=(10, 6))
sns.boxplot(data=data[numerical_cols])
plt.title('Boxplots for Numerical Features')
plt.show()

# Visualizing pairwise relationships between numerical features
sns.pairplot(data, hue='status', diag_kind='kde', markers=["o", "s"], vars=numerical_cols, height=2.5)
plt.suptitle('Pairwise Plot of Numerical Features', y=1.02)
plt.show()
# Calculating the correlation matrix for numerical features
corr_matrix = data[numerical_cols].corr()

# Correlation matrix heatmap with more styling
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, cbar_kws={'shrink': 0.75})
plt.title('Correlation Matrix of Numerical Features')
plt.show()
# Visualizing categorical columns with count plots (gender, workex, etc.)
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
sns.countplot(data['gender'], ax=axes[0, 0])
axes[0, 0].set_title('Gender Distribution')

sns.countplot(data['workex'], ax=axes[0, 1])
axes[0, 1].set_title('Work Experience Distribution')

sns.countplot(data['ssc_b'], ax=axes[1, 0])
axes[1, 0].set_title('SSC Board Distribution')

sns.countplot(data['hsc_b'], ax=axes[1, 1])
axes[1, 1].set_title('HSC Board Distribution')

plt.tight_layout()
plt.show()

# Encoding categorical columns using LabelEncoder
label_encoder = preprocessing.LabelEncoder()
data['gender'] = label_encoder.fit_transform(data['gender'])
data['ssc_b'] = label_encoder.fit_transform(data['ssc_b'])
data['hsc_b'] = label_encoder.fit_transform(data['hsc_b'])
data['hsc_s'] = label_encoder.fit_transform(data['hsc_s'])
data['degree_t'] = label_encoder.fit_transform(data['degree_t'])
data['workex'] = label_encoder.fit_transform(data['workex'])
data['specialisation'] = label_encoder.fit_transform(data['specialisation'])
data['status'] = label_encoder.fit_transform(data['status'])

# Final data overview
data.head()

# Seperating Features and Target
X = data.copy().drop('status', axis=1)
y = data['status']

# scale each features
X_scaled = preprocessing.scale(X)

#Train Test Split
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, y, test_size=0.3)
X_train.shape, Y_train.shape, X_test.shape

# K-Nearest Neighbor Model Training
knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(X_train, Y_train)  # Training the model with training data

# Making predictions on the test set
knn_predictions = knn_model.predict(X_test)

# Accuracy calculation
knn_accuracy = knn_model.score(X_test, Y_test)
print(f"KNN Model Accuracy: {knn_accuracy:.4f}")  # Display the accuracy score

# Confusion Matrix
conf_matrix = confusion_matrix(Y_test, knn_predictions)

# Plotting the confusion matrix as a heatmap
plt.figure(figsize=(10, 7))
conf_matrix_labels = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
conf_matrix_values = ["{0:0.0f}".format(val) for val in conf_matrix.flatten()]
formatted_labels = [f"{label}\n{value}" for label, value in zip(conf_matrix_labels, conf_matrix_values)]
formatted_labels = np.asarray(formatted_labels).reshape(2, 2)

sns.heatmap(conf_matrix, annot=formatted_labels, annot_kws={"size": 16}, fmt='', cmap="Blues")
plt.title("Confusion Matrix for KNN Classifier")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Precision, Recall, and F1-Score via classification report
print("\nClassification Report:")
print(classification_report(Y_test, knn_predictions))

# Support Vector Machine Model Training
svc_model = SVC()
svc_model.fit(X_train, Y_train)  # Fit the model with training data

# Predicting the outcomes on the test set
svm_predictions = svc_model.predict(X_test)

# Calculating the accuracy of the model
svc_accuracy = svc_model.score(X_test, Y_test)
print(f"SVM Model Accuracy: {svc_accuracy:.4f}")  # Display the accuracy

# Confusion Matrix
conf_matrix_svc = confusion_matrix(Y_test, svm_predictions)

# Plotting the confusion matrix as a heatmap
plt.figure(figsize=(10, 7))
conf_matrix_labels = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
conf_matrix_values = ["{0:0.0f}".format(val) for val in conf_matrix_svc.flatten()]
formatted_labels = [f"{label}\n{value}" for label, value in zip(conf_matrix_labels, conf_matrix_values)]
formatted_labels = np.asarray(formatted_labels).reshape(2, 2)

sns.heatmap(conf_matrix_svc, annot=formatted_labels, annot_kws={"size": 16}, fmt='', cmap="YlGnBu")
plt.title("Confusion Matrix for SVM Classifier")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Generating precision, recall, and F1-score metrics
print("\nSVM Classification Report:")
print(classification_report(Y_test, svm_predictions))

# Random Forest Model Training
random_forest_model = RandomForestClassifier(n_estimators=1000)
random_forest_model.fit(X_train, Y_train)  # Fit the model on the training data

# Making predictions on the test set
random_forest_predictions = random_forest_model.predict(X_test)

# Evaluating the model performance on both training and test sets
random_forest_train_accuracy = random_forest_model.score(X_train, Y_train)
random_forest_test_accuracy = random_forest_model.score(X_test, Y_test)
print(f"Random Forest Training Accuracy: {random_forest_train_accuracy:.4f}")
print(f"Random Forest Test Accuracy: {random_forest_test_accuracy:.4f}")

# Confusion Matrix
conf_matrix_rf = confusion_matrix(Y_test, random_forest_predictions)

# Plotting the confusion matrix as a heatmap
plt.figure(figsize=(10, 7))
conf_matrix_labels = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
conf_matrix_values = ["{0:0.0f}".format(val) for val in conf_matrix_rf.flatten()]
formatted_labels = [f"{label}\n{value}" for label, value in zip(conf_matrix_labels, conf_matrix_values)]
formatted_labels = np.asarray(formatted_labels).reshape(2, 2)

sns.heatmap(conf_matrix_rf, annot=formatted_labels, annot_kws={"size": 16}, fmt='', cmap="coolwarm")
plt.title("Confusion Matrix for Random Forest Classifier")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Generating classification report with precision, recall, and F1-score metrics
print("\nRandom Forest Classification Report:")
print(classification_report(Y_test, random_forest_predictions))

# Stochastic Gradient Descent Model Training
sgd_model = SGDClassifier()
sgd_model.fit(X_train, Y_train)  # Fit the model with training data

# Making predictions on the test set
sgd_predictions = sgd_model.predict(X_test)

# Calculating accuracy
sgd_accuracy = sgd_model.score(X_test, Y_test)
print(f"SGD Model Accuracy: {sgd_accuracy:.4f}")  # Display the accuracy

# Confusion Matrix
conf_matrix_sgd = confusion_matrix(Y_test, sgd_predictions)

# Plotting the confusion matrix as a heatmap
plt.figure(figsize=(10, 7))
conf_matrix_labels = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
conf_matrix_values = ["{0:0.0f}".format(val) for val in conf_matrix_sgd.flatten()]
formatted_labels = [f"{label}\n{value}" for label, value in zip(conf_matrix_labels, conf_matrix_values)]
formatted_labels = np.asarray(formatted_labels).reshape(2, 2)

sns.heatmap(conf_matrix_sgd, annot=formatted_labels, annot_kws={"size": 16}, fmt='', cmap="BuGn")
plt.title("Confusion Matrix for SGD Classifier")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Generating precision, recall, and F1-score metrics
print("\nSGD Classification Report:")
print(classification_report(Y_test, sgd_predictions))

# Creating a DataFrame with model names and their accuracy scores
models = pd.DataFrame({
    'Model': ['KNN', 'Linear SVC', 'Random Forest', 'SGD'],
    'Score': [knn_accuracy, svc_accuracy, random_forest_test_accuracy, sgd_accuracy] # Use random_forest_test_accuracy instead of predictions
})

# Sorting the models by accuracy score in descending order
models_sorted = models.sort_values(by='Score', ascending=False)

# Creating a horizontal dot plot to visualize model accuracy scores
plt.figure(figsize=(10, 6))
sns.stripplot(x='Score', y='Model', data=models_sorted, jitter=True, marker='o', size=10, color='green', alpha=0.7)
plt.title("Model Accuracy Comparison (Dot Plot)")
plt.xlabel("Accuracy Score")
plt.ylabel("Model")
plt.show()

from sklearn.ensemble import VotingClassifier

# Define base models
base_models = [
    ('knn', KNeighborsClassifier(n_neighbors=3)),
    ('svc', SVC(probability=True)),  # Ensure probability=True for VotingClassifier
    ('rf', RandomForestClassifier(n_estimators=1000)),
    ('sgd', SGDClassifier(loss='log_loss'))  # Using log loss to work with probabilities
]

# Create a voting classifier
voting_clf = VotingClassifier(estimators=base_models, voting='soft')  # Soft voting uses probability

# Train the voting classifier
voting_clf.fit(X_train, Y_train)

# Evaluate the model
voting_predictions = voting_clf.predict(X_test)
voting_accuracy = voting_clf.score(X_test, Y_test)

# Print classification report
print("\nVoting Classifier Accuracy:", voting_accuracy)
print("\nVoting Classifier Report:\n", classification_report(Y_test, voting_predictions))

# Confusion Matrix Visualization
conf_matrix_voting = confusion_matrix(Y_test, voting_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_voting, annot=True, fmt='d', cmap='coolwarm')
plt.title("Confusion Matrix - Voting Classifier")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Function to evaluate model performance
def evaluate_model(model, X_test, Y_test, model_name):
    predictions = model.predict(X_test)

    # Compute Metrics
    accuracy = accuracy_score(Y_test, predictions)
    precision = precision_score(Y_test, predictions)
    recall = recall_score(Y_test, predictions)
    f1 = f1_score(Y_test, predictions)
    roc_auc = roc_auc_score(Y_test, predictions)

    print(f"\n=== {model_name} Performance ===")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}")
    print(f"ROC-AUC: {roc_auc:.4f}")

    return [model_name, accuracy, precision, recall, f1, roc_auc]

# Evaluate Individual Models
knn_results = evaluate_model(knn_model, X_test, Y_test, "KNN")
svc_results = evaluate_model(svc_model, X_test, Y_test, "SVM")
rf_results = evaluate_model(random_forest_model, X_test, Y_test, "Random Forest")
sgd_results = evaluate_model(sgd_model, X_test, Y_test, "SGD")

# Evaluate Voting Classifier
voting_results = evaluate_model(voting_clf, X_test, Y_test, "Voting Classifier")

# Store results in a DataFrame
results_df = pd.DataFrame(
    [knn_results, svc_results, rf_results, sgd_results, voting_results],
    columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
)

# Sort by Accuracy
results_df = results_df.sort_values(by="Accuracy", ascending=False)

# Print the comparison table
print("\n=== Model Performance Comparison ===")
print(results_df)

# Visualize Performance
plt.figure(figsize=(10, 6))
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
for metric in metrics:
    sns.barplot(x='Model', y=metric, data=results_df, label=metric)

plt.legend(loc="best")
plt.title("Model Performance Comparison")
plt.xticks(rotation=30)
plt.show()